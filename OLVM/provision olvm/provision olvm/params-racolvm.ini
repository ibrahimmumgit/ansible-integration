#
#/* Copyright 2013-2019,  Oracle. All rights reserved. */
#
#
# WRITTEN BY: Oracle.
#  v1.8: Jun-2019 Enable NFS for clusterware storage
#  v1.7: Apr-2019 Direct NFS, SI/startup systemd
#  v1.6: Mar-2019 19c/OCI adjustments
#  v1.5: Oct-2018 Add OMF and pluggable tweaks
#  v1.4: Dec-2017 Disable ssh, firewall
#  v1.3: Nov-2017 18c adjustments
#  v1.2: Aug-2017 12.2 adjustments
#  v1.1: Jan-2015 12.1.0.2 adjustments, Multi ASM Diskgroup support
#  v1.0: Jul-2013 Creation
#                   
#
# Oracle DB/RAC 19c OneCommand - Generic configuration file
# For Single Instance, Single Instance HA (Oracle Restart) and Oracle RAC
#
#############################################
#
# Generic Parameters
#
# NOTE: The first section holds more advanced parameters that
#       should be modified by advanced users or if instructed by Oracle.
#
# See further down this file for the basic user modifiable parameters.
#
#############################################
#
# Temp directory (for OUI), optional
# Default: /tmp
TMPDIR="/tmp"
#
# Progress logfile location
# Default: OVM: $TMPDIR/progress-racovm.out
#          OCI: $TMPDIR/progress-dbrac.out
#LOGFILE="$TMPDIR/progress-racovm.out"
#
# Must begin with a "+", see "man 1 date" for valid date formats, optional.
# Default: "+%Y-%m-%d %T"
LOGFILE_DATE_FORMAT=""
#
# Should 'clone.pl' be used (default no) or direct 'attach home' (default yes)
# to activate the Grid & RAC homes.
# Attach is possible in the VM since all relinking was done already
# Certain changes may still trigger a clone/relink operation such as switching
# from role to non-role separation.
# Default: yes
CLONE_ATTACH_DBHOME=yes
CLONE_ATTACH_GIHOME=yes
#
# Should a re-link be done on the Grid & RAC homes. Default is no,
# since the software was relinked in VM already. Setting it to yes
# forces a relink on both homes, and overrides the clone/attach option
# above by forcing clone operation (clone.pl)
# Default: no
CLONE_RELINK=no
#
# Should a re-link be done on the Grid & RAC homes in case of a major
# OS change; Default is yes.  In case the homes are attached to a different
# major OS than they were linked against, a relink will be automatically
# performed.  For example, if the homes were linked on OL5 and then used
# with an OL6 OS, or vice versa, a relink will be performed. To disable
# this automated relinking during install (cloning step), set this
# value to no (not recommended)
# Default: yes
CLONE_RELINK_ON_MAJOR_OS_CHANGE=yes
#
# New in 12.2 and above it is possible to switch the Database Edition
# during deployment. Set CLONE_DATABASE_EDITION to 'STD' for Standard
# Edition 2 or 'EE' for Enterprise Edition (these are the only two valid
# values). Make sure the correct edition is used based on your license.
# Default: ""
CLONE_DATABASE_EDITION=EE
#
# The root of the oracle install must be an absolute path starting with a /
# Default: /u01/app
RACROOT="/u01/app"
#
# The location of the Oracle Inventory
# Default: $RACROOT/oraInventory
RACINVENTORYLOC="${RACROOT}/oraInventory"
#
# The location of the SOFTWARE base
# In role separated configuration GIBASE may be defined to set the location
# of the Grid home which defaults to $RACROOT/$GRIDOWNER.
# Default: $RACROOT/$RACOWNER
#RACBASE="${RACROOT}/oracle"
#
# The location of the Grid home, must be set in RAC or Single Instance HA deployments
# Default: $RACROOT/19c/grid
GIHOME="${RACROOT}/19c/grid"
#
# The location of the DB RAC home, must be set in non-Clusterware only deployments
# Default: ${RACBASE}/product/19c/dbhome_1
DBHOME="${RACROOT}/oracle/product/19c/dbhome_1"
#
# The disk string used to discover ASM disks, it should cover all disks
# on all nodes, even if their physical names differ. It can also hold
# ASMLib syntax, e.g. ORCL:VOL*, and have as many elements as needed
# separated by space, tab or comma.
# Do not remove the "set -/+o noglob" options below, they are required
# so that discovery string don't expand on assignment.
# Default: OVM: '/dev/xvd[c-g]1'
#          OCI: '/dev/nvme?n1p1'
set -o noglob
RACASMDISKSTRING='/dev/sd[a-t]1'
set +o noglob
#
# List of devices, actual partitions or ASMLib disks for the initial ASM diskgroup.
# For additional diskgroup support see ASM MULTI-DISKGROUP SUPPORT section below.
# If actual partition number is specified no repartitioning will be done, otherwise
# specify top level device name and the disk will automatically be partitioned with
# one partition using 'parted'. For example, if /dev/xvdh4 is listed
# below it will be used as is, if it does not exist an error will be raised.
# However, if /dev/xvdh is listed it will be automatically partitioned
# and /dev/xvdh1 will be used. ASMLib disks must be pre-created by the user manually.
# Minimum of 5 devices or partitions are recommended (see ASM_MIN_DISKS).
# Only used if CLONE_CLUSTER_STORAGE=ASM.
# Default: OVM: "/dev/xvdc /dev/xvdd /dev/xvde /dev/xvdf /dev/xvdg"
#          OCI: "/dev/nvme0n1"
ALLDISKS="/dev/sda"
#
# List of ASMLib disks to use for the initial ASM diskgroup.  Can be either
# "diskname" or "ORCL:diskname".  They must be manually configured in ASMLib by
# mapping them to correct block device (this part is not automated). Disk name
# given should be the ASMLib disk name, e.g. "DISK1" and not device name /dev/...
# If you include any disks here they should also be included
# in RACASMDISKSTRING setting above (discovery string).
# Since 2014 release it is possible to list ASMLib disks in ANY disk variable,
# such as ALLDISKS, RACASM_RECO_DISKS and RACASM_*_DISKS, hence ALLDISKS_ASMLIB
# is kept for backwards compatibility.
ALLDISKS_ASMLIB=""
#
# By default 5 disks for ASM are recommended to provide higher redundancy
# for OCR/Voting files. If for some reason you want to use less
# disks, then uncomment ASM_MIN_DISKS below and set to the new minimum.
# Make needed adjustments in ALLDISKS and/or ALLDISKS_ASMLIB above.
# Default: 5
ASM_MIN_DISKS=1
#
#       ---------------------------
# ----  ASM MULTI-DISKGROUP SUPPORT ----
#       ---------------------------
#    Starting with 2014 releases, multi-diskgroup support is added during initial
#    deployment and subsequent rebuild attempts.
#
#       REMINDER: Please follow the Best Practices on usage of diskgroups as
#       published by Oracle. For example, in general, one should not create a
#       diskgroup for each database or tablespace.
#       Diskgroup name is case insensitive (NaMe == NAME, always converted to UPPERCASE)
#       An ASM diskgroup name must be between 1 and 30 characters in length.
#       It must start with an alphabetical character and include only alphabetical
#       characters, numbers or the '_', '#', or '$' characters. Usage of '#' & '$'
#       is discouraged due to possible interoperability issues.
#
#    Automation is provided using the following new variables:
RACASMGROUPNAME_RECO='RECO'   # The name of the Recovery diskgroup (single name)
RACASM_RECO_REDUNDANCY="EXTERNAL" # Redundancy of the Recovery diskgroup: EXTERNAL, NORMAL (default), HIGH
RACASM_RECO_ATTRIBUTES="'compatible.asm'='19.0.0.0.0', 'compatible.rdbms'='19.0.0.0.0'" # Attributes of the Recovery diskgroup
RACASM_RECO_DISKS="/dev/sdb"      # List of disks used to create the Recovery diskgroup
#
RACASMGROUPNAME_EXTRA='DATA'  # List of names of additional diskgroups to be created
RACASM_1_REDUNDANCY="EXTERNAL"    # Redundancy of the Nth diskgroup (default: NORMAL)
RACASM_1_ATTRIBUTES="'compatible.asm'='19.0.0.0.0', 'compatible.rdbms'='19.0.0.0.0'"    # Attributes of the Nth diskgroup
RACASM_1_DISKS="/dev/sdc"         # List of disks used to create the Nth diskgroup

#RACASM_2_REDUNDANCY="HIGH"    # Redundancy of the Nth diskgroup (default: NORMAL)
#RACASM_2_ATTRIBUTES="'compatible.asm'='19.0.0.0.0', 'compatible.rdbms'='19.0.0.0.0'"    # Attributes of the Nth diskgroup
#RACASM_2_DISKS="/dev/xvdd /dev/xvde /dev/xvdf"         # List of disks used to create the Nth diskgroup
#
#CLONE_GIMR_SEPARATE_DISKGROUP=yes
#      Using single quotes (') for assignment of names to prevent shell expansion in
#      case a "$" exists in name
#
#      Diskgroup attribute syntax must use single quotes around attribute names and values,
#      separated by an equal sign, e.g. 'au_size'='2M', separate each such pair by comma
#      for additional attributes (entire string is enclosed in double quotes).
#
#      Example of multi-diskgroup setup:
#RACASMGROUPNAME_EXTRA='TEST MYDEV'  will attempt to create 2 extra
#          diskgroups, provided that RACASM_1_* and RACASM_2_* are set correctly.
#
#RACASM_RECO_ATTRIBUTES="'compatible.asm'='12.1.0.0.0', 'compatible.rdbms'='12.1.0.0.0'"
#          Will set asm & rdbms compatibile attribute for the Recovery diskgroup to 12.1
#RACASM_1_ATTRIBUTES="'compatible.asm'='12.1.0.0.0', 'compatible.rdbms'='12.1.0.0.0'"
#          Will set asm & rdbms compatibile attribute for the Nth diskgroup to 12.1
#
#    The original 'RACASMGROUPNAME' variable (defined in 'basic section' further down) still
#    holds the primary diskgroup name, if left unset, ASM will not be configured at all.
#    The original 'ALLDISKS' variable still holds all the disks used for the primary diskgroup.
#
#    Disk names must be UNIQUE amongst ALLDISKS, RACASM_RECO_DISKS and RACASM_X_DISKS (X=number)
#
# Set to YES to create diskgroups in the background and continue other operations.
# Default: no
#CLONE_DISKGROUP_CREATE_BG=no
#
#    The following variables allow various components to use the created diskgroups.
#    The diskgroups listed below must also be listed above (to be created), except MGMTDB_DISKGROUP.
#    If however they will be created manually or already exist, you must set:
#    CLONE_SKIP_DISKGROUP_EXISTENCE_CHECK=yes to allow execution to continue.
#ACFS_DISKGROUP=""         # Diskgroup for ACFS filesystem
DBCA_DISKGROUP="DATA"         # Diskgroup for Database
#MGMTDB_DISKGROUP="MGMT"       # Diskgroup for Management DB (12c and higher)
#
#
# By default, whole disks specified in ALLDISKS will be partitioned with
# one partition. If you prefer not to partition and use whole disk, set
# PARTITION_WHOLE_DISKS to no. Keep in mind that if at a later time
# someone will repartition the disk, data may be lost. Probably better
# to leave it as "yes" and signal it's used by having a partition created.
# Default: yes
PARTITION_WHOLE_DISKS=yes
#
# By default, disk *names* are assumed to exist with same name on all nodes, i.e
# all nodes will have /dev/xvdc, /dev/xvdd, etc.  It doesn't mean that the *ordering*
# is also identical, i.e. xvdc can really be xvdd on the other node.
# If such persistent naming (not ordering) is not the case, i.e node1 has
# xvdc,xvdd but node2 callsthem: xvdn,xvdm then PERSISTENT_DISKNAMES should be
# set to NO.  In the case where disks are named differently on each node, a
# stamping operation should take place (writing to second sector on disk)
# to verify if all nodes see all disks.
# Stamping only happens on the node the build is running from, and backup
# is taken to $TMPDIR/StampDisk-backup-diskname.dd. Remote nodes read the stamped
# data and if all disks are discovered on all nodes the disk configuration continues.
# Default: yes
PERSISTENT_DISKNAMES=no
#
# This parameter decides whether disk stamping takes place or not to discover and verify
# that all nodes see all disks.  Stamping is the only way to know 100% that the disks
# are actually the same ones on all nodes before installation begins.
# The master node writes a unique uuid to each disk on the second sector of the disk,
# then remote nodes read and discover all disks.
# If you prefer not to stamp the disks, set DISCOVER_VERIFY_REMOTE_DISKS_BY_STAMPING to
# no. However, in that case, PERSISTENT_DISKNAMES must be set to "yes", otherwise, with
# both parameters set to "no" there is no way to calculate the remote disk names.
# The default for stamping is "yes" since in Virtual machine environments, scsi_id(8)
# doesn't return data for disks.
# Default: yes
DISCOVER_VERIFY_REMOTE_DISKS_BY_STAMPING=yes
#
# Permissions and ownership file for UDEV
UDEVFILE="/etc/udev/rules.d/99-oracle.rules"
#
# Disk permissions to be set on ASM disks use if want to override the below default
# Default: "660" (owner+group: read+write)
#  It may be possible in Non-role separation to use "640" (owner: read+write, group: read)
#  however, that is not recommended since if a new database OS user
#  is added at a later time in the future, it will not be able to write to the disks.
#DISKPERMISSIONS="660"
#
# ASM's minimum allocation unit (au_size) for objects/files/segments/extents of the first
# diskgroup, in some cases increasing to higher values may help performance (at the
# potential of a bit of space wasting). Legal values are 1,2,4,8,16,32 and 64 MB.
# Not recommended to go over 8MB. Currently if initial diskgroup holds OCR/Voting then it's
# maximum possible au_size is 16MB. Do not change unless you understand the topic.
# Default 12.2: 4MB, older releases 1MB
#RACASM_AU_SIZE=4
#
# The same au_size can be set for the backup diskgroup (typically used to hold the GIMR data)
# Default 12.2: 4MB, older releases 1MB
#RACASM_BACKUP_AU_SIZE=4
#
# Should we align the ASM disks to a 1MB boundary.
# Default: yes
ALIGN_PARTITIONS=yes
#
# Should partitioned disks use the GPT or MSDOS partition table.
# GPT supports devices larger than 2TB. If unset will auto-switch to 'gpt' for large devices.
# Default: "" (use msdos partition table and switch to gpt for larger devices)
#PARTITION_TABLE_GPT=no
#
# These are internal functions that check if a disk/partition is held
# by any component. They are run in parallel on all nodes, but in sequence
# within a node. Do not modify these unless explicitly instructed to by Oracle.
# Default: HeldByRaid HeldByAsmlib HeldByPowerpath HeldByDeviceMapper HeldByUser HeldByFilesystem HeldBySwap
#HELDBY_FUNCTIONS=(HeldByRaid HeldByAsmlib HeldByPowerpath HeldByDeviceMapper HeldByUser HeldByFilesystem HeldBySwap)
#
#       ----------------------------
# ----  STORAGE: (Shared) Filesystem ----
#       ----------------------------
#
# NOTE: Not all operations/verification take place in a
#       FS configuration.
#  For example:
#   - The mount points are not automatically mounted
#   - Best effort verification is done that the correct
#     mount options are used.
#
# Release 19c or higher allow for Cluster Storage to reside on ASM or NAS
# (Network Attached Storage), also referred to as 'filesystem'.
# Valid values for Cluster Storage are "ASM" (on block devices) or "FILESYSTEM" (on NAS).
# When "ASM" is selected, then ALLDISKS and related variables should be set, when
# "FILESYSTEM" is selected, then CLONE_OCR_DISKS & CLONE_VOTING_DISKS should
# be set (see below)
# Default: ASM
CLONE_CLUSTER_STORAGE="ASM"
#
# The filesystem directory to hold Database files (control, logfile, etc.)
# For RAC it must be a shared location (NFS, OCFS or in 12c ACFS),
# otherwise it may be a local filesystem (e.g. ext4).
# For NFS make sure mount options are correct as per docs
# such as Note:359515.1
# Default: None (Single Instance: $RACBASE/oradata)
#FS_DATAFILE_LOCATION=/nfs/160
#
# Should the database be created in the FS location mentioned above.
# If value is unset or set to no, the database is created in ASM.
# Default: no (Single Instance: yes)
#DATABASE_ON_FS=no
#
# Should the above directory be cleared from Clusterware and Database
# files during a 'clean' or 'cleanlocal' operation.
# Default: no
#CLONE_CLEAN_FS_LOCATIONS=no
#
# Permissions of top level filesystem DB location, to skip this permission change
# set CLONE_SKIP_FS_LOCATION_PERMISSION_CHANGE=no
# Default: 770
#FSPERMISSIONS=770
#
# Names of OCR/VOTE disks, could be in above FS Datafile location
# or a different properly mounted (shared) filesystem location
# Default: None
#CLONE_OCR_DISKS=/nfs/160/ocr1,/nfs/160/ocr2,/nfs/160/ocr3
#CLONE_VOTING_DISKS=/nfs/160/vote1,/nfs/160/vote2,/nfs/160/vote3
#
# Should Database use Kernel NFS or Direct NFS; setting 'yes' will use Direct NFS
# Default: yes
#CLONE_DIRECT_NFS=yes
#
# Should addnodes operation COPY the entire Oracle Homes to newly added
# nodes. By default no copy is done to speed up the process, however
# if existing cluster members have changed (patches applied) compared
# to the newly created nodes (using the template), then a copy
# of the Oracle Homes might be desired so that the newly added node will
# get all the latest modifications from the current members.
# If COPY is set to yes, please consider that the copy does not remove the
# home on the newly joining node, either manually remove those,
# or set CLONE_ADDNODES_REMOVES_ORACLE_HOME_BEFORE_COPY=yes to automatically
# remove all Oracle homes before an addnode+Copy operation. Not removing
# the target homes may lead to unexpected behavior due to pre-existing files.
# Default: no
CLONE_ADDNODES_COPY=no
#
# Should an add node operation fully clean the new node before adding
# it to the cluster. Setting to yes means that any lingering running
# Oracle processes on the new node are killed before the add node is
# started as well as all logs/traces are cleared from that node.
# Default: no
CLONE_CLEAN_ON_ADDNODES=no
#
# Should a remove node operation fully clean the removed node after removing
# it from the cluster. Setting to yes means that any lingering running
# Oracle processes on the removed node are killed after the remove node is
# completed as well as all logs/traces are cleared from that node.
# Default: no
CLONE_CLEAN_ON_REMNODES=no
#
# Should 'cleanlocal' request prompt for confirmation if processes are running
# Note that a global 'clean' will fail if this is set to 'yes' and processes are running
# this is a designed safegaurd to protect environment from accidental removal.
# Default: yes
CLONE_CLEAN_CONFIRM_WHEN_RUNNING=yes
#
# Should the recommended oracle-validated or oracle-rdbms-server-*-preinstall
# be checked for existance and dependencies during check step. If any missing
# rpms are found user will need to use up2date or other methods to resolve dependencies
# The RPM may be obtained from Unbreakable Linux Network or http://oss.oracle.com
# Default: yes
#CLONE_ORACLE_PREREQ_RPM_REQD=yes
#
# Should the "verify" actions of the above RPM be run during buildcluster.
# These adjust kernel parameters. In the VM everything is pre-configured hence
# default is not to run.
# Default: no
#CLONE_ORACLE_PREREQ_RPM_RUN=no
#
# By default after clusterware installation CVU (Cluster Verification Utility)
# is executed to make sure all is well. Setting to 'yes' will skip this step.
# Set CLONE_SKIP_CVU_POSTHAS for SIHA (Oracle Restart) environments
# Default: no
#CLONE_SKIP_CVU_POSTCRS=no
#
# Allows to skip minimum disk space checks on the
# Oracle Homes (recommended not to skip)
# Default: no
#CLONE_SKIP_DISKSPACE_CHECKS=no
#
# Allows to skip minimum memory checks (recommended not to skip)
# Default: no
#CLONE_SKIP_MEMORYCHECKS=no
#
# On systems with extreme memory limitations, e.g. VirtualBox, it may be needed
# to disable some Clusterware components to release some memory. Workload
# Management, Cluster Health Monitor & Cluster Verification Utility are
# disabled if this option is set to yes.
# This is only supported for production usage with Clusterware only installation.
# Default: no
#CLONE_LOW_MEMORY_CONFIG=no
#
# By default on systems with less than 4GB of RAM the /dev/shm will
# automatically resize to fit the specified configuration (ASM, DB).
# This is done because the default of 50% of RAM may not be enough. To
# disable this functionality set CLONE_TMPFS_SHM_RESIZE_NEVER to YES
# Default: no
#CLONE_TMPFS_SHM_RESIZE_NEVER=no
#
# To disable the modification of /etc/fstab with the calculated size of
# /dev/shm, set CLONE_TMPFS_SHM_RESIZE_MODIFY_FSTAB to NO. This may mean that
# some instances may not properly start following a system reboot.
# Default: yes
#CLONE_TMPFS_SHM_RESIZE_MODIFY_FSTAB=yes
#
# Configures the Grid Infrastructure Managemet Repository (GIMR).
# Formerly known as Cluster Management DB or Cluster Health Monitor or CHM/OS.
# Starting with release 12.1.0.2 and higher must be configured by
# setting CLONE_GRID_MANAGEMENT_DB to yes, for a fully supported environment.
# GIMR's space requirement might be high, hence default is still set to NO.
# Default: no
CLONE_GRID_MANAGEMENT_DB=no
#
# Configure Trace File Analyzer or not
# Default: "no"
CLONE_TRACEFILE_ANALYZER="yes"
#
# Setting CLONE_CLUSTERWARE_ONLY to yes allows Clusterware only installation
# any operation to create a database or reference the DB home are ignored.
# Default: no
#CLONE_CLUSTERWARE_ONLY=no
#
# New in 12.2 Cluster Class can be set to one of:
#      STANDALONE, DOMAIN, MEMBERDB, MEMBERAPP
# Only STANDALONE & DOMAIN are automated.
#CLONE_CLUSTER_CLASS=STANDALONE
#
# As described in the 11.2.0.2 README as well as Note:1212703.1 mutlicasting
# is required to run Oracle RAC starting with 11.2.0.2. If this check fails
# review the note, and remove any firewall rules from Dom0, or re-configure
# the switch servicing the private network to allow multicasting from all
# nodes to all nodes.
# Default: yes
CLONE_MULTICAST_CHECK=yes
#
# Should a multicast check failure cause the build to stop. It's possible to
# perform the multicast check, but not stop on failures.
# Default: yes
CLONE_MULTICAST_STOP_ON_FAILURE=yes
#
# List of multicast addresses to check. By default 11.2.0.2 supports
# only 230.0.1.0, however with fix for bug 9974223 or bundle 1 and higher
# the software also supports multicast address 244.0.0.251. If future
# software releases will support more addresses, modify this list as needed.
# Default: "230.0.1.0 224.0.0.251"
CLONE_MULTICAST_ADDRESSLIST="230.0.1.0 224.0.0.251"
#
# Maximum number of seconds allowed for time drift between nodes.
# If CLONE_CHECK_CLOCK_DRIFT_STOP_ON_FAILURE (default no) is set to YES, then
# a larger time drift will be regarded as a failure and buildcluster will stop.
# Default: 30 (seconds)
#CLONE_CHECK_CLOCK_DRIFT=""
#
# The text specified in the NETCONFIG_RESOLVCONF_OPTIONS variable is written to
# the "options" field in the /etc/resolv.conf file during initial network setup.
# This variable can be set here in params.ini, or in netconfig.ini having the same
# effect. It should be a space separated options as described in "man 5 resolv.conf"
# under the "options" heading. Some useful options are:
# "single-request-reopen attempts:x timeout:x"  x being a digit value.
# The 'single-request-reopen' option may be helpful in some environments if
# in-bound ssh slowness occur.
# Note that minimal validation takes place to verify the options are correct.
# Default: ""
#NETCONFIG_RESOLVCONF_OPTIONS=""
#
# CLUSTERWARE limit names listed in CLONE_CRS_LIMIT_NAMES, by default:
#  "CRS_LIMIT_CORE,CRS_LIMIT_MEMLOCK,CRS_LIMIT_OPENFILE,CRS_LIMIT_STACK,CRS_LIMIT_NPROC"
# Will look for CLONE_<limitname> and place in GIHOME/crs/install/s_crsconfig_<nodename>_env.txt
# No input validation takes place, use with caution!
# For example: CLONE_CRS_LIMIT_OPENFILE=65535 (will set the open file limit to 65535)
#
# ORACLE USERS limits (in /etc/security/limits.conf and related files) are set by preinstall
# or oracle-validated rpms. If rpms are not installed (not recommended), or the setting was
# removed or not set there, the following variables can be used as fallback (defaults), they
# do not take precedence over the rpms.  Set CLONE_<soft|hard>_<limitname> to desired value.
# Valid limitnames: nofile nproc stack core memlock
# For example: CLONE_soft_core=unlimited (will set the soft limit for core to 'unlimited')
#
# By default the Oracle/Grid user environment is setup (PATH, ORACLE_HOME, ORACLE_SID)
# unless it was already setup before.  Setting to "yes" will skip this setup altogether.
# Default: "no"
#CLONE_SKIP_USER_ENVIRONMENT_SETUP="no"
#
# By default Oracle Universal Installer takes a backup of the inventory during
# each operation. Setting this to NO prevents these backups, useful for testing
# or larger node counts.
# Default: "yes"
#CLONE_BACKUP_INVENTORY="yes"
#
# Starting with 12.1.0.2 it is possible to set a list of IPs so that node VIP can
# properly failover in the event of a network outage that may be "masked" by the
# underlying virtualization layer. It is also possible to set/modify post install:
#  $GIHOME/bin/srvctl modify network -pingtarget "..."
# Default: ""
#CLONE_PING_TARGETS=""
#
#################################################
#
# The second section below holds Basic parameters
#
#################################################
#
# Configures a Single Instance environment, including a database as
# specified in BUILD_SI_DATABASE. In this mode, no Clusterware or ASM will be
# configured, hence all related parameters (e.g. ALLDISKS) are not relevant.
# The database must reside on a filesystem.
# This parameter may be placed in netconfig.ini for simpler deployment.
# Default: no
#CLONE_SINGLEINSTANCE=no
#
# Configures a Single Instance/HA environment, aka Oracle Restart, including
# a database as specified in BUILD_SI_DATABASE. The database may reside in
# ASM (if RACASMGROUPNAME is defined), or on a filesystem.
# This parameter may be placed in netconfig.ini for simpler deployment.
# Default: no
#CLONE_SINGLEINSTANCE_HA=no
#
# OS USERS AND GROUPS FOR ORACLE SOFTWARE
#
# SYNTAX for user/group are either (VAR denotes the variable names below):
#   VAR=username:uid   OR:  VAR=username
#                           VARID=uid
#   VAR=groupname:gid  OR:  VAR=groupname
#                           VARID=gid
#
#   If uid/gid are omitted no checks are made nor users created if need be.
#   If uid/gid are supplied they should be numeric and not clash
#   with existing uid/gids defined on the system already.
#   NOTE: In RAC usernames and uid/gid must match on all cluster nodes,
#         the verification process enforces that only if uid/gid's
#         are given below.
#
# If incorrect configuration is detected, changes to users and groups are made to
# correct them. If this is set to "no" then errors are reported
# without an attempt to fix them.
# (Users/groups are never dropped, only added or modified.)
# Default: yes
CREATE_MODIFY_USERS_GROUPS=yes
#
# NON-ROLE SEPARATED:
#    No Grid user is defined and all roles are set to 'dba'
#RACOWNER=oracle:54321
#OINSTALLGROUP=oinstall:54321
#GIOSASM=dba:54322
#GIOSDBA=dba:54322
#GIOSOPER=dba:54322   # optional in 12c
#DBOSDBA=dba:54322
#DBOSOPER=dba:54322  # optional in 12c
#
# ROLE SEPARATION: (uncomment lines below)
#    See Note:1092213.1
#    (Numeric changes made to uid/gid to reduce the footprint and possible clashes
#     with existing users/groups)
#
GRIDOWNER=grid:54322
RACOWNER=oracle:54321
OINSTALLGROUP=oinstall:54321
DBOSDBA=dba:54322
DBOSOPER=oper:54323 # optional in 12c
## New in 12c are these 3 roles, if unset, they default to "DBOSDBA"
#DBOSBACKUPDBA=backupdba:54324
#DBOSDGDBA=dgdba:54325
#DBOSKMDBA=kmdba:54326
#DBOSRACDBA=racdba:54330    # New in 12.2
GIOSDBA=asmdba:54327
GIOSOPER=asmoper:54328 # optional in 12c
GIOSASM=asmadmin:54329
#
# The name for the Grid home in the inventory
# Default: OraGI19Home1
#GIHOMENAME="OraGI19Home1"
#
# The name for the DB/RAC home in the inventory
# Default: OraDB19Home1
#DBHOMENAME="OraDB19Home1"
#
# The name of the initial ASM diskgroup, default 'ocrvfdg'
# If set to an empty string "", ASM will not be configured (see STORAGE/Filesystem section above)
# For additional diskgroup support see ASM MULTI-DISKGROUP SUPPORT section above.
# Default: ocrvfdg
RACASMGROUPNAME='OCR'
#
# Attributes for the default/initial ASM diskgroup specified in RACASMGROUPNAME. There is no
# validation for this setting; same syntax as diskgroup attributes in above storage section.
# Default: release dependent
RACASMGROUP_ATTRIBUTES="'compatible.asm'='19.0.0.0.0', 'compatible.rdbms'='19.0.0.0.0'"
#
# The ASM Redundancy for the diskgroup above
# Valid values are EXTERNAL, NORMAL or HIGH
# Default: NORMAL (if more than 5 devices in ALLDISKS, otherwise EXTERNAL)
RACASMREDUNDANCY="EXTERNAL"
#
# Allows running the Clusterware with a different timezone than the system's timezone.
# If CLONE_CLUSTERWARE_TIMEZONE is not set, the Clusterware Timezone will
# be set to the system's timezone of the node running the build.  System timezone is
# defined in /etc/sysconfig/clock (ZONE variable), if not defined or file missing
# comparison of /etc/localtime file is made against the system's timezone database in
# /usr/share/zoneinfo, if no match or /etc/localtime is missing GMT is used. If you
# want to override the above logic, simply set CLONE_CLUSTERWARE_TIMEZONE to desired
# timezone. Note that a complete timezone is needed, e.g. "PST" or "EDT" is not enough
# needs to be full timezone spec, e.g. "PST8PDT" or "America/New_York".
# This variable is only honored in 11.2.0.2 or above
# Default: OS
CLONE_CLUSTERWARE_TIMEZONE="Asia/Jakarta"
#
# Allows configuration with or without ASM Filter Driver (ASMFD) kernel drivers
# If unset, best-effort to detect if AFD is supported or not on running Kernel,
# during initial deployment
# Default: yes (auto)
CLONE_AFD_ENABLED=yes
#
# Create an ACFS volume?
# Default: no
ACFS_CREATE_FILESYSTEM=no
#
# If ACFS volume is to be created, this is the mount point.
# It will automatically get created on all nodes.
# Default: /myacfs
ACFS_MOUNTPOINT="/myacfs"
#
# Name of ACFS volume to optionally create.
# Default: MYACFS
ACFS_VOLNAME="MYACFS"
#
# Size of ACFS volume in GigaBytes.
# Default: 3
ACFS_VOLSIZE_GB="3"
#
# NOTE: In the OVM3 enhanced RAC Templates when using deploycluster
# tool (outside of the VMs). The correct and secure way to transfer/set the
# passwords is to remove them from this file and use the -P (--params)
# flag to transfer this params.ini during deploy operation, in which
# case the passwords will be prompted, and sent to all VMs in a secure way.
# The password that will be set for the ASM and RAC databases
# as well as EM DB Console and the oracle OS user.
# If not defined here they will be prompted for (only once)
# at the start of the build. Required to be set here or environment
# for silent mode.
# Use single quote to prevent shell parsing of special characters.
RACPASSWORD='Mandiri123'
GRIDPASSWORD='Mandiri123'
#
# Password for 'root' user. If not defined here it will be prompted
# for (only once) at the start of the build.
# Assumed to be same on both nodes and required to be set here or
# environment for silent mode.
# Use single quote to prevent shell parsing of special characters.
ROOTUSERPASSWORD='Mandiri123'
#
# Size of REDO logfiles in MB. Minimum 4MB.
# Default: None (use DBCA's default, changes per machine size/release)
REDOLOG_FILESIZE_MB="2048"
#
# Allows for database and listener to be started automatically at next
# system boot. This option is only applicable in Single Instance mode.
# In Single Instance/HA or RAC mode, the Clusterware starts up all
# resources (listener, ASM, databases).
# Default: yes
#CLONE_SI_DATABASE_AUTOSTART=yes
#
# Should above mentioned automated startup use systemd (available on OL7)
# or traditional /etc/rc.local
# Default: no
#CLONE_SI_DATABASE_AUTOSTART_USE_SYSTEMD=no
#
# Comma separated list of name value pairs for database initialization parameters
# Use with care, no validation takes place.
# For example: "db_create_online_log_dest_1='+EXTRA',db_create_online_log_dest_2='+DATA'"
#  Will create multiplexed Redo logs and Controlfiles into EXTRA & DATA diskgroups
#  assuming these diskgroups are created properly.
# Can be used to set NLS parameters and so forth.
# Default: none
#DBCA_INITORA_PARAMETERS=""
#
# Create a Container Database allowing pluggable databases to be added
# using options below, or at a later time.
# Default: yes (was 'no' in prior releases)
DBCA_CONTAINER_DB=no
#
# Pluggable Database name. In 'createdb' operation a number is appended at the end
# based on count (below). In 'deletepdb' exact name must be specified here or in
# an environment variable.
# Default: orclpdb
#DBCA_PLUGGABLE_DB_NAME=orclpdb
#
# Number of Pluggable Databases to create during a 'createdb' operation. A value
# of zero disables pluggable database creation.
# Default: 1
DBCA_PLUGGABLE_DB_COUNT=0
#
# Should a Policy Managed database be created taking into account the
# options below. If set to 'no' an Admin Managed database is created.
# Default: no
#DBCA_DATABASE_POLICY=no
#
# Create Server Pools (Policy Managed database).
# If set to NO, the server pools must be manually created
# Default: yes
CLONE_CREATE_SERVERPOOLS=yes
#
# Recreate Server Pools; if already exist (Policy Managed database).
# Default: no
CLONE_RECREATE_SERVERPOOLS=no
#
# List of server pools to create (Policy Managed database).
# Syntax is poolname:category:min:max
# All except name can be omitted. Category can be Hub or Leaf.
# Default: mypool
CLONE_SERVERPOOLS="mypool"
#
# List of Server Pools to be used by the created database (Policy Managed database).
# The server pools listed in DBCA_SERVERPOOLS must appear in CLONE_SERVERPOOLS
# (and CLONE_CREATE_SERVERPOOLS set to yes), OR must be manually pre-created for
# the create database to succeed.
# Default: mypool
DBCA_SERVERPOOLS="mypool"
#
# Database character set (no input validation). Can be AL32UTF8, WE8MSWIN1252, etc.
# Default: AL32UTF8
#DATABASE_CHARACTERSET="AL32UTF8"
#
# Database national character set. Rarely needs to be changed, the default of
# AL16UTF16 is recommended. All other NLS related init.ora should go above in
# DBCA_INITORA_PARAMETERS.
# Default: AL16UTF16
#DATABASE_NATIONAL_CHARACTERSET=""
#
# Use this DBCA template name, file must exist under $DBHOME/assistants/dbca/templates
# Default: "General_Purpose.dbc"
#DBCA_TEMPLATE_NAME=""
#
# Should the database include the sample schema
# Default: no
#DBCA_SAMPLE_SCHEMA=no
#
# Registers newly created database to be periodically monitored by Cluster Verification
# Utility (CVU) on a continuous basis.
# Default: no
DBCA_RUN_CVU_PERIODICALLY=no
#
# Certain patches applied to the Oracle home require execution of some SQL post
# database creation for the fix to be applied completely. Note that when running
# in background these scripts may run a little longer after the Database
# finishes building.
# Default: yes
#DBCA_POST_SQL_BG=yes
#
# An optional user custom SQL may be executed post database creation, default name of
# script is user_custom_postsql.sql, it is located under patches/postsql subdirectory.
# Default: user_custom_postsql.sql
DBCA_POST_SQL_CUSTOM=user_custom_postsql.sql
#
# Total memory assigned to instance in MB.
# Default: version dependent
#DBCA_TOTALMEMORY=1024
#
# Decides if Oracle Managed Files are used or not. It is implicitly enabled
# when ASM is used. Setting to true allows using OMF when the database is
# stored on a filesystem.
#
# Default: Automatic (based on storage)
#DBCA_USEOMF=true
#
# The Database Name
# Default: ORCL
DBNAME='DEVDB'
#
# The Instance name, may be different than database name. Limited in length of
# 1 to 8 for a RAC DB & 1 to 12 for Single Instance DB of alphanumeric characters.
# Ignored for Policy Managed DB.
# Default: ORCL
SIDNAME='DEVDB'
#
# Build Database? The BUILD_RAC_DATABASE will build a RAC database and
# BUILD_SI_DATABASE a single instance database (also in a RAC environment)
# Default: yes
BUILD_RAC_DATABASE=yes
#BUILD_SI_DATABASE=yes
#
# Configure EM DB Express
# Default: no
#CONFIGURE_DBEXPRESS=no
#
# DB Express port number. If left at the default, a free port will be assigned at
# runtime, otherwise the port should be unused on all network adapters.
# Default: 5500
#DBEXPRESS_HTTPS_PORT=5500
#
# SCAN (Single Client Access Name) port number
# Default: 1521
SCANPORT=1522
#
# Local Listener port number
# Default: 1521
LISTENERPORT=1522
#
# By default, at end of deployment or clean operation, the ssh key-based authorization keys
# are removed, however, if these should be kept as-is, set CLONE_CLEAN_REMOVES_SSH=no
# Default: yes
#CLONE_CLEAN_REMOVES_SSH=yes
#
# By default, in RAC mode, during 'prepare' step, the firewall is disabled to allow free traffic
# as per note: 554781.1. To disable that behavior, set CLONE_DISABLE_FIREWALL=no below.
# Default: yes
#CLONE_DISABLE_FIREWALL=yes
#
# Allows color coding of log messages, errors (red), warning (yellow),
# info (green). By default no colors are used.
# Default: NO
CLONE_LOGWITH_COLORS=yes
#
# END OF FILE
#

